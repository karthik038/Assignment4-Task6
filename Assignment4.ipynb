{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYNSl57hOfedXlnoxB9C6V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik038/Assignment4-Task6/blob/main/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement"
      ],
      "metadata": {
        "id": "98Mb0k2vQm91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting Median value of owner-occupied homes\n"
      ],
      "metadata": {
        "id": "zzUtNyI7QrdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this assignment is to learn the application of machine learning algorithms to data sets. This involves learning what data means, how to handle data, training, cross validation, prediction, testing your model, etc."
      ],
      "metadata": {
        "id": "3OeDPmc8QwvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive, and has been used extensively throughout the literature to benchmark algorithms. The data was originally published by Harrison, D. and Rubinfeld, D.L. Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978."
      ],
      "metadata": {
        "id": "9MXo3EsLQ3Xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is small in size with only 506 cases. It can be used to predict the median value of a home, which is done here. There are 14 attributes in each case of the dataset. They are:"
      ],
      "metadata": {
        "id": "CjKZp71kQ69s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. CRIM - per capita crime rate by town\n",
        "2. ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "3. INDUS - proportion of non-retail business acres per town.\n",
        "4. CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
        "5. NOX - nitric oxides concentration (parts per 10 million)\n",
        "6. RM - average number of rooms per dwelling\n",
        "7. AGE - proportion of owner-occupied units built prior to 1940\n",
        "8. DIS - weighted distances to five Boston employment centres\n",
        "9. RAD - index of accessibility to radial highways\n",
        "10. TAX - full-value property-tax rate per $10,000\n",
        "11. PTRATIO - pupil-teacher ratio by town\n",
        "12. B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "13. LSTAT - % lower status of the population\n",
        "14. MEDV - Median value of owner-occupied homes in $1000's"
      ],
      "metadata": {
        "id": "oiNkTw6gQ_Cj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gm6SCyjWMY9X"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Importing phi and phi_test from train and test datasets"
      ],
      "metadata": {
        "id": "rADr7krNM5-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phi = np.loadtxt('train.csv', dtype='float', delimiter=',', skiprows=1,usecols=tuple(range(1, 14)))\n",
        "\n",
        "phi_test = np.loadtxt('test.csv', dtype='float', delimiter=',',skiprows=1, usecols=tuple(range(1, 14)))"
      ],
      "metadata": {
        "id": "VdEJTGYyM3n-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Importing y from train data set"
      ],
      "metadata": {
        "id": "8OsVnOnBOMfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.loadtxt('train.csv', dtype='float', delimiter=',', skiprows=1,usecols=14, ndmin=2)"
      ],
      "metadata": {
        "id": "OCe7E5DVM-VA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Concatenating coloumn of 1s to right of phi and phi_test\n",
        "\n"
      ],
      "metadata": {
        "id": "AydgJq6rOZvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phi_test = np.concatenate((phi_test, np.ones((105, 1))), axis=1)\n",
        "\n",
        "phi = np.concatenate((phi, np.ones((400, 1))), axis=1)"
      ],
      "metadata": {
        "id": "vu_sBwjEOTAk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi_test.shape\n",
        "phi.shape"
      ],
      "metadata": {
        "id": "p9Mk8tubOeS3",
        "outputId": "b03d70fe-2684-4c4b-f83a-cf1fe5086d2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "LU_chBJ9Oll8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Min max scaling on each coloumn of phi and phi_test\n"
      ],
      "metadata": {
        "id": "YvG9Z0BPOtF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(0, 13):\n",
        "    col_max = max(phi[:, x])\n",
        "    col_min = min(phi[:, x])\n",
        "    phi[:, x] = (phi[:, x] - col_min) / (col_max - col_min)\n",
        "    phi_test[:, x] = (phi_test[:, x] - col_min) / (col_max - col_min)"
      ],
      "metadata": {
        "id": "ii7u28CEOgi4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Applying log scaling on y"
      ],
      "metadata": {
        "id": "zfWjHnCpO6Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.log(y)"
      ],
      "metadata": {
        "id": "PMDtxmiZO3i2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Function to calculate change in error function based on phi, w and p norm"
      ],
      "metadata": {
        "id": "vSuEyy3vPHJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delta_w(p, phi, w):\n",
        "    if p == 2:\n",
        "        deltaw = (2 * (np.dot(np.dot(np.transpose(phi), phi), w) -\n",
        "                       np.dot(np.transpose(phi), y)) +\n",
        "                  lamb * p * np.power(np.absolute(w), (p - 1)))\n",
        "    if p < 2 and p > 1:\n",
        "        deltaw = (2 * (np.dot(np.dot(np.transpose(phi), phi), w) -\n",
        "                       np.dot(np.transpose(phi), y)) +\n",
        "                  lamb * p * np.power(np.absolute(w), (p - 1)) * np.sign(w))\n",
        "    return deltaw"
      ],
      "metadata": {
        "id": "p-6eMdmoPIyV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Dictionary containing filenames as keys and p as values"
      ],
      "metadata": {
        "id": "XYBmHR_IPPEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_names = {'output.csv': 2.0,\n",
        "             'output_p1.csv': 1.75,\n",
        "             'output_p2.csv': 1.5,\n",
        "             'output_p3.csv': 1.3\n",
        "             }"
      ],
      "metadata": {
        "id": "uFUfzLm-PLks"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For each item in this dictionary"
      ],
      "metadata": {
        "id": "KMvPcjXpPWcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (fname, p) in file_names.items():\n",
        "\n",
        "    # Set initial w to zeros\n",
        "    w = np.zeros((14, 1))\n",
        "\n",
        "    # Hyperparameter lambda value (trail and error)\n",
        "    lamb = 0.2\n",
        "\n",
        "    # Maximum step size\n",
        "    t = 0.00012\n",
        "\n",
        "    # Calculate new value of w\n",
        "    w_new = w - t * delta_w(p, phi, w)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    # Repeat steps until error between consecutive w is less than threshold\n",
        "    while(np.linalg.norm(w_new-w) > 10 ** -10):\n",
        "        w = w_new\n",
        "        w_new = w - t * delta_w(p, phi, w)\n",
        "        i = i + 1\n",
        "\n",
        "    # Load values of id from the data file\n",
        "    id_test = np.loadtxt('test.csv', dtype='int', delimiter=',',\n",
        "                         skiprows=1, usecols=0, ndmin=2)\n",
        "\n",
        "    # Calculate y for test data using phi test and applying inverse log\n",
        "    y_test = np.exp(np.dot(phi_test, w_new))\n",
        "\n",
        "    # Save the ids and y according to filename from dictionary\n",
        "    np.savetxt(fname, np.concatenate((id_test, y_test), axis=1),\n",
        "               delimiter=',', fmt=['%d', '%f'], header='ID,MEDV', comments='')"
      ],
      "metadata": {
        "id": "u0GugeI6PT6w"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}